---
title: "HW2 STA521 Fall18"
author: 'Ziwei Zhu zz169 sophiazzw7'
date: "Due September 24, 2018 9am"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Backgound Reading

Readings: Chapters 3-4 in Weisberg Applied Linear Regression


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This exercise involves the UN data set from `alr3` package. Install `alr3` and the `car` packages and load the data to answer the following questions adding your code in the code chunks.  Please add appropriate code to the chunks to suppress messages and warnings as needed once you are sure the code is working properly and remove instructions if no longer needed. Figures should have informative captions. Please switch the output to pdf for your final version to upload to Sakai. **Remove these instructions for final submission**


## Exploratory Data Analysis



```{r data}
include = FALSE
suppressWarnings(library(car))
suppressWarnings(library(carData))
library(carData)
library(alr3)
data(UN3, package="alr3")
help(UN3) 
library(car)
library(ggplot2)
library(knitr)
```


1. Create a summary of the data.  How many variables have missing data?  Which are quantitative and which are qualtitative?

```{r}
summary(UN3)
str(UN3)
```
All of the variables except Purban have missing values. All of the vairables are quantitative vaariables.


2. What is the mean and standard deviation of each quantitative predictor?  Provide in a nicely formatted table. 

```{r}

sd = as.data.frame(round(apply(UN3[1:7],2,sd,na.rm=TRUE),3))
mean = as.data.frame(round(apply(UN3[1:7],2,mean,na.rm=TRUE),3))
sdMean_table = cbind(sd,mean)
rm(sd,mean)
colnames(sdMean_table) = c('SD','Mean')
kable(sdMean_table)

sd = as.data.frame(round(apply(UN3[,1:7],2,sd,na.rm=TRUE),3))
mean = as.data.frame(round(apply(UN3[,1:7],2,mean,na.rm=TRUE),3))
sdMean_table = cbind(sd,mean)
rm(sd,mean)
colnames(sdMean_table) = c('SD','Mean')
kable(sdMean_table)


```


3. Investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots
highlighting the relationships among the predictors. Comment
on your findings regarding trying to predict `ModernC` from the other variables.  Are there potential outliers, nonlinear relationships or transformations that appear to be needed based on your graphical EDA?

```{r}

UN3_0 = na.omit(UN3)
library(GGally)
ggpairs(UN3_0, progress = FALSE,title = "Pairing comparison on qualitative variables")
pairs(UN3_0)

par(mfrow = c(2, 2))
plot(log(UN3$ModernC), UN3$change)
plot(sqrt(UN3$ModernC), UN3$PPgdp)
plot((UN3$ModernC)^2, UN3$Fertility)
plot(log(UN3$ModernC), UN3$Pop)
```
ModernC had nonlinear relationship with Pop. Pop and ModernC has some potential outliers.

## Model Fitting

4.  Use the `lm()` function to perform a multiple linear regression with `ModernC` as the response and all other variables as the predictors, using the formula `ModernC ~ .`, where the `.` includes all remaining variables in the dataframe.  Create  diagnostic residual plot from the linear model object and comment on results regarding assumptions.  How many observations are used in your model fitting?

```{r}
coef(lm(ModernC ~ . , data= UN3_0))
anova(lm(ModernC ~ . , data= UN3_0))
reg <- lm(ModernC ~ . , data= UN3_0)
summary(reg)
par(mfrow = c(2, 2))
plot(reg)
```
r=residual randomly distributed arount fitted line, we saw the residual vs fitted graph looks fine. The normal QQ plot is showing a straight line trend rather than a curved shape,so we saw not necessiliy normality with a few outliers on the tails. heavy-tailed,quantile larger than normal value
We wanted to see random pattern in the scale-location plot, and we kind of have it.
For the residual vs.leverage plot, we could see India and China being two influential points. 125 obeservations are used in my model fitting.

5. Examine added variable plots `car::avPlot` or `car::avPlots`  for your model above. Are there any plots that suggest that transformations are needed for any of the terms in the model? Describe. Is it likely that any of the localities are influential for any of the terms?  Which localities?  Which terms? 

```{r}
car::avPlots(model = reg)
```
From the added-variable plots, i think the Pop is especially clustered and did not show linearlity, also the PPgdp shows a little clustering pattern, so i think transformation is needed for Pop and PPgdp.

From the graphs, Kuwaito and Cook's Islands are influential for Change. China and India are influential for Pop.

6.  Using the Box-Tidwell  `car::boxTidwell` or graphical methods find appropriate transformations of the predictor variables to be used as predictors in the linear model.  If any predictors are negative, you may need to transform so that they are non-negative.  Describe your method and the resulting transformations.


```{r}

  
car::boxTidwell(ModernC~PPgdp+Pop,other.x=~Frate+Change+Fertility+Purban,data=UN3,max.iter=25, tol=0.001,verbose=FALSE)

powerTransform(as.matrix(UN3_0)~.,family="bcnPower",data=UN3_0)
range(UN3_0['Change'])
UN3_1=UN3_0
UN3_1['Change']=UN3_0['Change']+2
powerTransform(as.matrix(UN3_1)~.,family="bcnPower",data=UN3_1)
UN3_trans=UN3_1
UN3_trans['Pop']=log(UN3_1['Pop'])
UN3_trans['PPgdp']=sqrt(UN3_1['PPgdp'])
reg_trans=lm(ModernC~.,data=UN3_trans)
par(mfrow = c(2, 2))
avPlots(reg_trans)
plot(reg_trans)
termplot(reg , terms = "Pop",
         partial.resid = T, se=T, rug=T,
         smooth = panel.smooth)
termplot(reg_trans , terms = "Pop",
         partial.resid = T, se=T, rug=T,
         smooth = panel.smooth)


car::boxTidwell(ModernC~PPgdp+Pop,other.x=~Frate+Change+Fertility+Purban,data=UN3_trans,max.iter=25, tol=0.001,verbose=FALSE)

```

I initially looked at the added variable plots and saw PPgdp and Pop may be two variable needing transformation(since they are both clustered). I first tried the boxTidwell method and found that both variables give insigificance, Then i tries the powerTransform function, found that Change should be transformed to its 0.3 power, however, we would want to elimate the negative values in the variable Change. After bringing all values in Change positive, i apply powerTranform again to find that only variable needing transformation is Pop. Since 0.33 is relatively close to 0.5, a square root transformation will be approriate. Then i tried the boxTidwell again to find that a log transformation on PPgdp will be disired.



7. Given the selected transformations of the predictors, select a transformation of the response using `MASS::boxcox` or `car::boxCox` and justify.


```{r}

car::boxCox(reg_trans,family="yjPower",plotit=TRUE)

```
I decided not to impose any transformation on ModernC the response variable since lamda interval includes 1.


8.  Fit the regression using the transformed variables.  Provide residual plots and added variables plots and comment.  If you feel that you need additional transformations of either the response or predictors, repeat any steps until you feel satisfied.

```{r}


reg_trans=lm(ModernC~.,data=UN3_trans)
par(mfrow = c(2, 2))
avPlots(reg_trans)
plot(reg_trans)
termplot(reg , terms = "Pop",
         partial.resid = T, se=T, rug=T,
         smooth = panel.smooth)
termplot(reg_trans , terms = "Pop",
         partial.resid = T, se=T, rug=T,
         smooth = panel.smooth)
termplot(reg , terms = "PPgdp",
         partial.resid = T, se=T, rug=T,
         smooth = panel.smooth)
termplot(reg_trans , terms = "PPgdp",
         partial.resid = T, se=T, rug=T,
         smooth = panel.smooth)
```
After the transformation, the termsplot shows that both PPgdp and Pop are less clustered. We could also see this pattern from the added variable plot. The residual plot did not show a lot difference before and after transformation.

9. Start by finding the best transformation of the response and then find transformations of the predictors.  Do you end up with a different model than in 8?


```{r}
boxCox(reg,family="yjPower",plotit=TRUE)
reg_test <- lm(ModernC~.,data=UN3_0)
car::avPlots(reg_test)
powerTransform(as.matrix(UN3_0)~.,family="bcnPower",data=UN3_0)
range(UN3_0['Change'])
UN3_1['Change']=UN3_0['Change']+2
powerTransform(as.matrix(UN3_1)~.,family="bcnPower",data=UN3_1)
UN3_trans1=UN3_1
UN3_trans1['Pop']=UN3_1['Pop']^0.5
UN3_trans1['PPgdp']=log(UN3_1['PPgdp'])
reg_trans1=lm(ModernC~.,data=UN3_trans1)
par(mfrow = c(2, 2))
avPlots(reg_trans1)
plot(reg_trans1)
termplot(reg , terms = "Pop",
         partial.resid = T, se=T, rug=T,
         smooth = panel.smooth)
termplot(reg_trans1 , terms = "Pop",
         partial.resid = T, se=T, rug=T,
         smooth = panel.smooth)


```
No, because no transformation has been imposed on the response variable, i end up with the same model as in question 8.



10.  Are there any outliers or influential points in the data?  Explain.  If so, refit the model after removing any outliers and comment on residual plots.


```{r}
UN3_out=UN3_1[-c(25,50),]
reg_rm<-lm(ModernC ~ log(PPgdp)+sqrt(Pop)+Change+Frate+Fertility+Purban,data=UN3_out)
par(mfrow = c(2, 2))
plot(reg_rm)
car::avPlots(reg_rm)
summary(reg_rm)
```
China and India are points with high leverage and they are potential outliers, but not necessilary influential points. After removing these two points, another new point came to our eyes, Poland, marked by R, which is not that high leverage in comparison to China and India.



## Summary of Results

11. For your final model, provide summaries of coefficients with 95% confidence intervals in a nice table with interpretations of each coefficient.  These should be in terms of the original units! 


```{r}



summary(reg_rm)$coefficient
a=as.matrix(summary(reg_rm)$coefficient)
b=data.frame("Estimate"=a[,1],"Lower Confidence Interval"=(a[,1]-a[,2]),"Upper Confidence Interval"=(a[,1]+a[,2]))
kable(b)


# M<-confint(reg_rm)
# #M[2,1]<-exp(confint(reg_rm,level=0.95)[2,1])
# #M[2,2]<-exp(confint(reg_rm,level=0.95)[2,2])
# M[3,1]<-confint(reg_rm)[3,1]^2
# M[3,2]<-confint(reg_rm)[3,2]^2
# #row.names(M)[2]="PPgdp"
# row.names(M)[3]="Pop"
# round(M,3)

# summary(reg)
# M<-confint(reg,level=0.95)
# #M[2,1]<-exp(confint(reg_rm,level=0.95)[2,1])
# #M[2,2]<-exp(confint(reg_rm,level=0.95)[2,2])
# #M[3,1]<-confint(reg_1,level=0.95)[3,1]^2
# #M[3,2]<-confint(reg_rm,level=0.95)[3,2]^2
# #row.names(M)[2]="PPgdp"
# #row.names(M)[3]="Pop"
# round(M,5)

```
One unit increse in square root of per capita 2001 GDP will result in 5.183 unit increase in percent of unmarried women using a modern method of contraception.And the 95% confidence interval is [3.81,6.55]

One unit increse in log of population(in thousands) will result in 0.0236 unit increase in percent of unmarried women using a modern method of contraception.And the 95% confidence interval is [0.012,0.035]

One unit increse in annual population growth rate percent will result in 4.84 unit increase in percent of unmarried women using a modern method of contraception.And the 95% confidence interval is [2.78,6.89]

One unit increse in percent of demales over 15 economically active will result in 0.182 unit increase in percent of unmarried women using a modern method of contraception.And the 95% confidence interval is [0.104,0.259]

One unit increse in expected number of life births per female 2000 will result in -9.31 unit increase in percent of unmarried women using a modern method of contraception.And the 95% confidence interval is [-11,-7.5]

One unit increse in Percent of population that is urban, 2001 will result in -0.29 unit increase in percent of unmarried women using a modern method of contraception.And the 95% confidence interval is [-0.127,0.068]


12. Provide a paragraph summarizing your final model  and findings suitable for the US envoy to the UN after adjusting for outliers or influential points.   You should provide a justification for any case deletions in your final model


I use na.omit to remove all rows containing NA's also, i removed two potential influential points:India and China. After all these case deletions, i applied log transformation to Per capital GDP and square root transformation to Population. And the final model ModernC~Change+log(PPgdp)+Frate+sqrt(Pop)+Fertility+Purban. And my finding is after applying these transformations, the added-variable plots shows that the Population is not so clustered.




## Methodology

    
13. Prove that the intercept in the added variable scatter plot will always be zero.  _Hint:  use the fact that if $H$ is the project matrix which contains a column of ones, then $1_n^T (I - H) = 0$.  Use this to show that the sample mean of residuals will always be zero if there is an intercept.

$$e_{(y)}=\hat{\beta_0}+\hat{\beta_1}e_{(x)}$$
$$(I-H)y=\hat{\beta_0}+\hat{\beta_1}(I-H)X_{i}$$
We know that $$\hat{beta_1}=(X^TX)^{-1}X^Ty, \text{and} X=(I-H)x_{i},y=(I-H)y$$
Thus, $$(I-H)y=\hat{\beta_0}*\mathbb{1}(I-H)x_{i}$$
and $$\hat{\beta_1}=(x^Tx)^{-1}x^Ty$$, where $$x=(I-H)x_{i},y=(I-H)y$$
so, we have $$(I-H)y=\hat{\beta_0}\mathbb{1}+[x_i^T(I-H)(I-H)X_i]^{-1}((I-H)X_i)^T(I-H)y(I-H)X_i$$
$$(I-H)y=\hat{\beta_0}\mathbb{1}+[x_i^T(I-H)X_i]^{-1}x_i^T(I-H)y(I-H)x_i$$
$$x_i^T(I-H)y=x_i^T\hat{\beta_0}\mathbb{1}+x_i^T[x_i^T(I-H)X_i]^{-1}x_i^T(I-H)y(I-H)x_i$$

$$x_i^T(I-H)y=x_i^T\mathbb{1}\hat{\beta_0}+x_i^T(I-H)x_i[x_i^T(I-H)X_i]^{-1}x_i^T(I-H)y$$
$$x_i^T(I-H)y=\sum_{j=1}^{n}x_{ij}\hat{\beta_0}+x_i^T(I-H)y$$
Thus, we have $$\sum_{j=1}^{n}x_{ij}\hat{\beta_0}=0$$
And since$\sum_{j=1}^{n}x_{ij}$ is a constant, we know $\hat{\beta_0}=0$.




14. For multiple regression with more than 2 predictors, say a full model given by `Y ~ X1 + X2 + ... Xp`   we create the added variable plot for variable `j` by regressing `Y` on all of the `X`'s except `Xj` to form `e_Y` and then regressing `Xj` on all of the other X's to form `e_X`.  Confirm that the slope in a manually constructed added variable plot for one of the predictors  in Ex. 10 is the same as the estimate from your model. 

```{r}
e_Y = residuals(lm(ModernC~log(PPgdp)+Frate+log(Pop)+Change+Purban, data=UN3_out))
e_X1 = residuals(lm(Fertility ~ log(PPgdp)+Frate+log(Pop)+Change+Purban, data=UN3_out))
df = data.frame(e_Y=e_Y, e_X1=e_X1)
ggplot(data=df, aes(x = e_X1, y = e_Y)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
summary(reg_rm)$coef
summary(lm(e_Y ~ e_X1, data=df))$coef

```

